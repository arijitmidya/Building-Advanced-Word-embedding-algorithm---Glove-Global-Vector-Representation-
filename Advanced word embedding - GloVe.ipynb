{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d47f97be-1450-4609-9cc9-a9aa2639e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from matplotlib import pylab\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e7530-44f1-4091-b9eb-dce16ab6ae80",
   "metadata": {},
   "source": [
    "# 1. Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc597388-70d8-4a2f-b7db-377a85169758",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
    "\n",
    "\n",
    "def download_data(url, data_dir):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "\n",
    "    # Create the data directory if it does not exist\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
    "\n",
    "    # If file doesnt exist, download\n",
    "    if not os.path.exists(file_path):\n",
    "        print('Downloading file...')\n",
    "        filename, _ = urlretrieve(url, file_path)\n",
    "    else:\n",
    "        print(\"File already exists\")\n",
    "\n",
    "    extract_path = os.path.join(data_dir, 'bbc')\n",
    "\n",
    "    # If data has not been extracted already, extract data\n",
    "    if not os.path.exists(extract_path):\n",
    "        with zipfile.ZipFile(os.path.join(data_dir, 'bbc-fulltext.zip'), 'r') as zipf:\n",
    "            zipf.extractall(data_dir)\n",
    "    else:\n",
    "        print(\"bbc-fulltext.zip has already been extracted\")\n",
    "\n",
    "download_data(url, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b5e95-d566-4f2a-9754-380d9012a1d5",
   "metadata": {},
   "source": [
    "# 2. Reading data without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247e14bd-d10b-41f0-bddb-d7891384c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. 401.txt\n",
      "Detected 2225 stories\n",
      "865163 words found in the total news set\n",
      "Example words (start):  Ad sales boost Time Warner profit  Quarterly profi\n",
      "Example words (end):  Online was the game, ahhhh them was the days ! LOL\n"
     ]
    }
   ],
   "source": [
    "def read_data(data_dir):\n",
    "\n",
    "    # This will contain the full list of stories\n",
    "    news_stories = []\n",
    "\n",
    "    print(\"Reading files\")\n",
    "\n",
    "    i = 0 # Just used for printing progress\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "\n",
    "        for fi, f in enumerate(files):\n",
    "\n",
    "            # We don't read the README file\n",
    "            if 'README' in f:\n",
    "                continue\n",
    "\n",
    "            # Printing progress\n",
    "            i += 1\n",
    "            print(\".\"*i, f, end='\\r')\n",
    "\n",
    "            # Open the file\n",
    "            with open(os.path.join(root, f), encoding='latin-1') as f:\n",
    "\n",
    "                story = []\n",
    "                # Read all the lines\n",
    "                for row in f:\n",
    "\n",
    "                    story.append(row.strip())\n",
    "\n",
    "                # Create a single string with all the rows in the doc\n",
    "                story = ' '.join(story)\n",
    "                # Add that to the list\n",
    "                news_stories.append(story)\n",
    "\n",
    "        print('', end='\\r')\n",
    "\n",
    "    print(f\"\\nDetected {len(news_stories)} stories\")\n",
    "    return news_stories\n",
    "\n",
    "\n",
    "news_stories = read_data(os.path.join('data', 'bbc'))\n",
    "\n",
    "# Printing some stats and sample data\n",
    "print(f\"{sum([len(story.split(' ')) for story in news_stories])} words found in the total news set\")\n",
    "print('Example words (start): ',news_stories[0][:50])\n",
    "print('Example words (end): ',news_stories[-1][-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3849d6-ae02-4b4b-9be3-c3a0b37677fa",
   "metadata": {},
   "source": [
    "# 3. Building a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af89241b-5199-4f6e-9823-bd15156baae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitted on the tokenizer\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "n_vocab = 15000 + 1\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=n_vocab - 1,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' ', oov_token=''\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(news_stories)\n",
    "print(\"Data fitted on the tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a28c27f-61bc-4a99-b0f9-0ecfce2976ab",
   "metadata": {},
   "source": [
    "# 4. Generate the cooc matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41d11fc2-232c-484f-86bc-3d0902e25e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................... 2200/2225\n",
      "\n",
      "It took 283.9533791542053 seconds to generate the co-occurrence matrix\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import save_npz, load_npz\n",
    "\n",
    "def generate_cooc_matrix(text, tokenizer, window_size, n_vocab, use_weighting=True):\n",
    "\n",
    "    # Convert list of text to list of list of word IDs\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "    # A sparse matrix to retain co-occurrences of words\n",
    "    cooc_mat = lil_matrix((n_vocab, n_vocab), dtype=np.float32)\n",
    "\n",
    "    # Go through each sequence one by one\n",
    "    for si, sequence in enumerate(sequences):\n",
    "\n",
    "        # Printing the progress\n",
    "        if (si+1)%100==0:\n",
    "            print('.'*((si+1)//100), f\"{si+1}/{len(sequences)}\", end='\\r')\n",
    "\n",
    "        # For each target word,\n",
    "        for i, wi in zip(np.arange(window_size, len(sequence)-window_size), sequence[window_size:-window_size]):\n",
    "\n",
    "            # Get the context window word IDs\n",
    "            context_window = sequence[i-window_size: i+window_size+1]\n",
    "\n",
    "            # The weight for the words in the context window (except target word) will be 1\n",
    "            window_weights = np.ones(shape=(window_size*2 + 1,), dtype=np.float32)\n",
    "            window_weights[window_size] = 0.0\n",
    "\n",
    "            if use_weighting:\n",
    "                # If weighting is used, penalize context words based on distance to target word\n",
    "                distances = np.abs(np.arange(-window_size, window_size+1))\n",
    "                distances[window_size] = 1.0\n",
    "                # Update the sparse matrix\n",
    "                cooc_mat[wi, context_window] += window_weights/distances\n",
    "            else:\n",
    "                # Update the sparse matrix\n",
    "                cooc_mat[wi, context_window] += window_weights\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return cooc_mat\n",
    "\n",
    "# ----------------------------------------- IMPORTANT ---------------------------------------------- #\n",
    "#                                                                                                    #\n",
    "# Set this true or false, depending on whether you want to generate the matrix or reuse the existing #\n",
    "#                                                                                                    #\n",
    "# ---------------------------------------------------------------------------------------------------#\n",
    "generate_cooc = True\n",
    "\n",
    "# Generate the matrix\n",
    "if generate_cooc:\n",
    "    t1 = time.time()\n",
    "    cooc_mat = generate_cooc_matrix(news_stories, tokenizer, 1, n_vocab, True)\n",
    "    t2 = time.time()\n",
    "    print(f\"It took {t2-t1} seconds to generate the co-occurrence matrix\")\n",
    "\n",
    "    save_npz(os.path.join('data','cooc_mat.npz'), cooc_mat.tocsr())\n",
    "# Load the matrix from disk\n",
    "else:\n",
    "    try:\n",
    "        cooc_mat = load_npz(os.path.join('data','cooc_mat.npz')).tolil()\n",
    "        print(f\"Cooc matrix of type {type(cooc_mat).__name__} was loaded from disk\")\n",
    "    except FileNotFoundError as ex:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not find the co-occurrence matrix on the disk. Did you generate the matrix by setting generate_cooc=True?\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b7143a-8001-4411-a139-ecaf945d0a9d",
   "metadata": {},
   "source": [
    "# 5. Defining the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16d6ab13-d1b6-44ad-9930-167341af8f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096 # Data points in a single batch\n",
    "\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "window_size=1 # We use a window size of 1 on either side of target word\n",
    "\n",
    "epochs = 5 # Number of epochs to train for\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "# We sample valid data points randomly from a large window without always being deterministic\n",
    "valid_window = 250\n",
    "\n",
    "# When selecting valid examples, we select some of the most frequent words as well as\n",
    "# some moderately rare words\n",
    "np.random.seed(54321)\n",
    "random.seed(54321)\n",
    "\n",
    "valid_term_ids = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_term_ids = np.append(\n",
    "    valid_term_ids, random.sample(range(1000, 1000+valid_window), valid_size),\n",
    "    axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499fe052-0c3a-4fda-af3e-878a18018a85",
   "metadata": {},
   "source": [
    "# 6. Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76672e24-bd68-45ae-b3a7-f4c1b7f6c19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ariji\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"glove_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"glove_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ target_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920,128</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ context_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920,128</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ target_embedding… │\n",
       "│                     │                   │            │ context_embeddin… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ target_embedding_b… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">15,001</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ context_embedding_… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">15,001</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dot[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│                     │                   │            │ target_embedding… │\n",
       "│                     │                   │            │ context_embeddin… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m)            │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m)            │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ target_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m1,920,128\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ context_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m1,920,128\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot (\u001b[38;5;33mDot\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ target_embedding… │\n",
       "│                     │                   │            │ context_embeddin… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ target_embedding_b… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │     \u001b[38;5;34m15,001\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ context_embedding_… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │     \u001b[38;5;34m15,001\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ dot[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│                     │                   │            │ target_embedding… │\n",
       "│                     │                   │            │ context_embeddin… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,870,258</span> (14.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,870,258\u001b[0m (14.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,870,258</span> (14.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,870,258\u001b[0m (14.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Embedding, Dot, Add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Define two input layers for context and target words\n",
    "word_i = Input(shape=())\n",
    "word_j = Input(shape=())\n",
    "\n",
    "# Each context and target has their own embeddings (weights and biases)\n",
    "\n",
    "# Embedding weights\n",
    "embeddings_i = Embedding(n_vocab, embedding_size, name='target_embedding')(word_i)\n",
    "embeddings_j = Embedding(n_vocab, embedding_size, name='context_embedding')(word_j)\n",
    "\n",
    "# Embedding biases\n",
    "b_i = Embedding(n_vocab, 1, name='target_embedding_bias')(word_i)\n",
    "b_j = Embedding(n_vocab, 1, name='context_embedding_bias')(word_j)\n",
    "\n",
    "# Compute the dot product between embedding vectors (i.e., w_i.w_j)\n",
    "ij_dot = Dot(axes=-1)([embeddings_i,embeddings_j])\n",
    "\n",
    "# Add the biases (i.e., w_i.w_j + b_i + b_j )\n",
    "pred = Add()([ij_dot, b_i, b_j])\n",
    "\n",
    "# The final model\n",
    "glove_model = Model(inputs=[word_i, word_j],outputs=pred, name='glove_model')\n",
    "\n",
    "# Glove has a specific loss function with a sound mathematical underpinning\n",
    "# It is a form of mean squared error\n",
    "glove_model.compile(loss=\"mse\", optimizer = 'adam')\n",
    "\n",
    "glove_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e43d4fb1-83b6-4f15-9979-b31e21066082",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sequences = tokenizer.texts_to_sequences(news_stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d230d03f-7ddd-4783-b67c-ef2d429358dd",
   "metadata": {},
   "source": [
    "# 7. Defining the Data Generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c084d05c-b09c-40ba-bd09-cc598f0c8162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([11701,  1792,  3188,  3520, 11382,  1326,  2541,   217,  2214,\n",
      "        5193]), array([    1,  1814,   403,   138,  3434, 13637,     7,   137,     2,\n",
      "         828]))\n",
      "[0.6931472 0.6931472 0.6931472 0.6931472 0.        0.6931472 1.0986123\n",
      " 0.6931472 2.3978953 0.6931472]\n",
      "[0.03162277 0.03162277 0.03162277 0.03162277 0.         0.03162277\n",
      " 0.05318296 0.03162277 0.17782794 0.03162277]\n"
     ]
    }
   ],
   "source": [
    "def glove_data_generator(\n",
    "    sequences, window_size, batch_size, vocab_size, cooccurrence_matrix, x_max=100.0, alpha=0.75, seed=None\n",
    "):\n",
    "    \"\"\" Generate batches of inputs and targets for GloVe \"\"\"\n",
    "\n",
    "    # Shuffle the data so that in every epoch, the order of data is different.\n",
    "    rand_sequence_ids = np.arange(len(sequences))\n",
    "    np.random.shuffle(rand_sequence_ids)\n",
    "\n",
    "    # We will use a sampling table to make sure we don't oversample stop words\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    # For each story/article\n",
    "    for si in rand_sequence_ids:\n",
    "\n",
    "        # Generate positive skip-grams while using subsampling\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequences[si],\n",
    "            vocabulary_size=vocab_size,\n",
    "            window_size=window_size,\n",
    "            negative_samples=0.0,\n",
    "            shuffle=False,\n",
    "            sampling_table=sampling_table,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        # Take targets and context words separately\n",
    "        targets, context = zip(*positive_skip_grams)\n",
    "        targets, context = np.array(targets).ravel(), np.array(context).ravel()\n",
    "\n",
    "\n",
    "        x_ij = np.array(cooccurrence_matrix[targets, context].toarray()).ravel()\n",
    "\n",
    "        # Compute log - Introducing an additive shift to make sure we don't compute log(0)\n",
    "        log_x_ij = np.log(x_ij + 1)\n",
    "\n",
    "        # Sample weights\n",
    "        # if x < x_max => (x/x_max)**alpha / else => 1\n",
    "        sample_weights = np.where(x_ij < x_max, (x_ij/x_max)**alpha, 1)\n",
    "\n",
    "        # If seed is not provided, generate a random one\n",
    "        if not seed:\n",
    "            seed = random.randint(0, 10e6)\n",
    "\n",
    "        # Shuffle data\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(context)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(targets)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(log_x_ij)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(sample_weights)\n",
    "\n",
    "        # Generate a batch or data in the format\n",
    "        # ((target words, context words), log(X_ij) <- true targets, f(X_ij) <- sample weights)\n",
    "        for eg_id_start in range(0, context.shape[0], batch_size):\n",
    "            yield (\n",
    "                targets[eg_id_start: min(eg_id_start+batch_size, targets.shape[0])],\n",
    "                context[eg_id_start: min(eg_id_start+batch_size, context.shape[0])]\n",
    "            ), log_x_ij[eg_id_start: min(eg_id_start+batch_size, x_ij.shape[0])], \\\n",
    "            sample_weights[eg_id_start: min(eg_id_start+batch_size, sample_weights.shape[0])]\n",
    "\n",
    "\n",
    "# Generate some data\n",
    "news_glove_data_gen = glove_data_generator(\n",
    "    news_sequences, 2, 10, n_vocab, cooc_mat\n",
    ")\n",
    "\n",
    "for x, y, z in news_glove_data_gen:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(z)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b4ed4b-0dce-4486-b932-0019ee7297b2",
   "metadata": {},
   "source": [
    "# 8. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce506eed-4d20-4c78-b559-b7f2ab6a0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, valid_term_ids, model_with_embeddings, tokenizer):\n",
    "\n",
    "        self.valid_term_ids = valid_term_ids\n",
    "        self.model_with_embeddings = model_with_embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\" Validation logic \"\"\"\n",
    "\n",
    "        # We will use context embeddings to get the most similar words\n",
    "        # Other strategies include: using target embeddings, mean embeddings after avaraging context/target\n",
    "        embedding_weights = self.model_with_embeddings.get_layer(\"context_embedding\").get_weights()[0]\n",
    "        normalized_embeddings = embedding_weights / np.sqrt(np.sum(embedding_weights**2, axis=1, keepdims=True))\n",
    "\n",
    "        # Get the embeddings corresponding to valid_term_ids\n",
    "        valid_embeddings = normalized_embeddings[self.valid_term_ids, :]\n",
    "\n",
    "        # Compute the similarity between valid_term_ids and all the embeddings\n",
    "        # V x d (d x D) => V x D\n",
    "        top_k = 5 # Top k items will be displayed\n",
    "        similarity = np.dot(valid_embeddings, normalized_embeddings.T)\n",
    "\n",
    "        # Invert similarity matrix to negative\n",
    "        # Ignore the first one because that would be the same word as the probe word\n",
    "        similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: top_k+1]\n",
    "\n",
    "        # Print the output\n",
    "        for i, term_id in enumerate(valid_term_ids):\n",
    "\n",
    "            similar_word_str = ', '.join([self.tokenizer.index_word[j] for j in similarity_top_k[i, :] if j > 1])\n",
    "            print(f\"{self.tokenizer.index_word[term_id]}: {similar_word_str}\")\n",
    "\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4587ccc9-9e3d-4b13-9887-a2a5f15823ed",
   "metadata": {},
   "source": [
    "# 9. Running the GloVe algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd3d3b64-dc73-4175-bc5b-15150d489bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 started\n",
      "   2224/Unknown \u001b[1m39s\u001b[0m 17ms/step - loss: 1.0483election: attorney, forthcoming, coming, studio, manager\n",
      "me: him, come, broadband, likely, japan\n",
      "with: formal, held, press, between, through\n",
      "you: we, they, also, still, doing\n",
      "were: are, was, but, because, wanted\n",
      "win: time, based, its, charge, level\n",
      "those: time, charge, governments, its, place\n",
      "music: cameras, growing, leaders, areas, our\n",
      "also: it, now, there, like, they\n",
      "third: first, own, take, according, club\n",
      "best: actor, growing, won, former, result\n",
      "him: come, me, charge, another, working\n",
      "too: so, how, better, stronger, any\n",
      "some: its, her, broadband, came, their\n",
      "through: them, into, set, door, could\n",
      "mr: tony, gordon, jack, article, closest\n",
      "file: illegally, systems, licensed, star, administration\n",
      "pair: them, come, working, broadband, looking\n",
      "ceremony: time, playing, go, use, his\n",
      "believed: took, said, being, wanted, were\n",
      "post: case, came, around, chelsea, time\n",
      "indian: children, kings, board, universities, kingfisher\n",
      "successful: some, controversial, popular, or, earlier\n",
      "care: put, took, criticised, hoping, come\n",
      "russia: place, came, put, another, 2005\n",
      "talk: talking, users, my, debate, charge\n",
      "programs: based, its, broadband, came, wales\n",
      "fair: results, saying, version, country's, christmas\n",
      "hollywood: jewish, day's, soldier, or, environmental\n",
      "attempt: likely, get, york, him, adds\n",
      "leave: pay, really, mci, whether, why\n",
      "light: biggest, level, time, world's, based\n",
      "\n",
      "\n",
      "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 17ms/step - loss: 1.0479\n",
      "Epoch: 2/5 started\n",
      "      4/Unknown \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0644"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ariji\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2224/Unknown \u001b[1m38s\u001b[0m 17ms/step - loss: 0.0415election: forthcoming, attorney, upcoming, manager, motors\n",
      "me: likely, part, come, aimed, him\n",
      "with: formal, tally, electro, uphill, held\n",
      "you: we, they, still, afford, detained\n",
      "were: are, because, saying, was, when\n",
      "win: based, least, brit, largest, victory\n",
      "those: driven, backed, took, important, showed\n",
      "music: cameras, subscriber, assistants, divide, revolution\n",
      "also: give, now, made, which, going\n",
      "third: fourth, take, seventh, statement, first\n",
      "best: supporting, category, actor, superhero, result\n",
      "him: come, part, place, me, working\n",
      "too: pretty, how, so, better, stronger\n",
      "some: compared, end, kind, way, came\n",
      "through: able, door, mci, outstanding, them\n",
      "mr: article, bernie, resignation, gordon, 63\n",
      "file: illegally, systems, catchy, abilities, solana\n",
      "pair: come, them, broadband, won, offered\n",
      "ceremony: driven, politics, rise, baby, least\n",
      "believed: wanted, because, fact, ensure, says\n",
      "post: grew, immediate, forced, podcasts, career\n",
      "indian: children, kings, let, parts, novel\n",
      "successful: visible, territory, center, browsing, competitive\n",
      "care: put, responsibility, bests, shot, close\n",
      "russia: way, result, put, part, 2004\n",
      "talk: talking, worried, least, pessimistic, part\n",
      "programs: program, hacker, based, wales, spread\n",
      "fair: begin, governing, version, 3g, results\n",
      "hollywood: day's, jewish, revolution, result, jupiter\n",
      "attempt: increase, effort, likely, use, lot\n",
      "leave: pay, mci, u, connected, against\n",
      "light: bridges, amplify, unveils, continuous, biggest\n",
      "\n",
      "\n",
      "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 17ms/step - loss: 0.0415\n",
      "Epoch: 3/5 started\n",
      "   2225/Unknown \u001b[1m39s\u001b[0m 18ms/step - loss: 0.0165election: forthcoming, attorney, empt, labour's, motors\n",
      "me: appeal, likely, him, aimed, part\n",
      "with: electro, tally, passport, atlantic, uphill\n",
      "you: they, we, afford, critics, sorry\n",
      "were: are, saying, was, when, because\n",
      "win: victory, least, sophos, clinton, logs\n",
      "those: because, ensure, goes, backed, involved\n",
      "music: cameras, subscriber, divide, assistants, mp3\n",
      "also: probably, give, play, it, help\n",
      "third: fourth, take, long, takes, seventh\n",
      "best: supporting, category, actor, fastest, makers\n",
      "him: me, part, came, aimed, likely\n",
      "too: pretty, bigger, better, how, interference\n",
      "some: end, kind, urban, compared, way\n",
      "through: door, tuned, soar, able, to\n",
      "mr: resignation, bernie, gordon, hughes, article\n",
      "file: illegally, systems, cambridge, catchy, battlefront\n",
      "pair: foot, built, named, along, moved\n",
      "ceremony: brit, politics, baby, phonographic, rise\n",
      "believed: wanted, because, fact, revealed, wants\n",
      "post: podcasts, grew, renaissance, forced, needed\n",
      "indian: southwest, enjoys, kingfisher, inadequate, small\n",
      "successful: popular, competitive, visible, beneficial, scary\n",
      "care: put, long, responsibility, relationship, urban\n",
      "russia: connectivity, series, practices, fall, 2005\n",
      "talk: talking, worried, bullish, least, concerned\n",
      "programs: hackers, program, hacker, architecture, appeal\n",
      "fair: partners, michael's, governing, version, 3g\n",
      "hollywood: day's, revolution, mgm, result, magazines\n",
      "attempt: lot, increase, likely, use, effort\n",
      "leave: pay, guaranteed, â£35bn, connected, able\n",
      "light: bridges, amplify, continuous, unveils, flowers\n",
      "\n",
      "\n",
      "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 18ms/step - loss: 0.0165\n",
      "Epoch: 4/5 started\n",
      "   2223/Unknown \u001b[1m38s\u001b[0m 17ms/step - loss: 0.0123election: forthcoming, attorney, empt, labour's, enter\n",
      "me: appeal, start, unable, compete, aimed\n",
      "with: gizmondo, sing, uphill, relaxation, tally\n",
      "you: we, they, afford, god, goodness\n",
      "were: are, saying, includes, reviewed, was\n",
      "win: victory, write, pressure, provinces, compete\n",
      "those: because, realising, died, others, criticised\n",
      "music: cameras, subscriber, refuseniks, mp3, upping\n",
      "also: probably, trying, walkouts, play, going\n",
      "third: fourth, long, close, takes, seventh\n",
      "best: supporting, category, superman, superhero, fastest\n",
      "him: rewarded, along, came, come, build\n",
      "too: pretty, bigger, better, stronger, interference\n",
      "some: create, hawkins, kinds, compared, example\n",
      "through: to, tuned, drives, dissolve, fingerprints\n",
      "mr: article, bernie, wes, 42, gordon\n",
      "file: illegally, systems, catchy, beirut, tapping\n",
      "pair: fact, foot, proposes, truth, along\n",
      "ceremony: baby, politics, brit, academy, sealing\n",
      "believed: fact, claimed, revealed, insisted, carpenter\n",
      "post: iraq, bidding, podcasts, beirut, grew\n",
      "indian: kingfisher, enjoys, sues, build, shake\n",
      "successful: popular, benefited, insulting, competitive, visible\n",
      "care: mobility, put, responsibility, powys, bests\n",
      "russia: october, rise, beginning, fall, lot\n",
      "talk: talking, worried, bullish, concerned, example\n",
      "programs: hackers, hacker, program, tool, detection\n",
      "fair: fagin, results, governing, member's, fold\n",
      "hollywood: day's, delhi, revolution, godzilla, mgm\n",
      "attempt: chance, lot, increase, return, likely\n",
      "leave: guaranteed, pay, able, â£35bn, door\n",
      "light: bridges, amplify, continuous, unveils, europe's\n",
      "\n",
      "\n",
      "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 17ms/step - loss: 0.0123\n",
      "Epoch: 5/5 started\n",
      "   2223/Unknown \u001b[1m38s\u001b[0m 17ms/step - loss: 0.0107election: forthcoming, attorney, campbell's, november's, labour's\n",
      "me: appeal, aimed, unable, wait, start\n",
      "with: reopen, reservoir, gizmondo, tally, together\n",
      "you: we, they, god, afford, goodness\n",
      "were: are, mori, saying, revealed, shouldn't\n",
      "win: victory, exploded, exit, pressure, zurich\n",
      "those: died, because, smokers, yougov, saying\n",
      "music: cameras, refuseniks, divide, mp3, subscriber\n",
      "also: walkouts, trigger, going, play, trying\n",
      "third: fourth, long, short, close, vital\n",
      "best: supporting, category, superhero, actor, fastest\n",
      "him: aimed, comply, cope, along, leap\n",
      "too: pretty, bigger, interference, better, happier\n",
      "some: kinds, create, democracy, use, sort\n",
      "through: to, tuned, drives, ahead, fingerprints\n",
      "mr: wes, resignation, cherie, bernie, article\n",
      "file: illegally, systems, catchy, tapping, acre\n",
      "pair: truth, fact, course, connected, possibility\n",
      "ceremony: brit, baby, sealing, perform, blazin'\n",
      "believed: added, gibbons, bjorn, said, carpenter\n",
      "post: iraq, podcasts, bidding, immense, merry\n",
      "indian: kingfisher, sues, enjoys, screaming, build\n",
      "successful: simpler, popular, benefited, expansive, important\n",
      "care: bests, put, function, algorithms, mobility\n",
      "russia: rise, october, 2005, practices, fall\n",
      "talk: talking, bullish, pessimistic, worried, example\n",
      "programs: hackers, hacker, program, loaded, tool\n",
      "fair: partners, ability, member's, california, lovers\n",
      "hollywood: day's, mgm, magazines, revolution, film's\n",
      "attempt: chance, increase, lot, couple, integral\n",
      "leave: guaranteed, pay, able, â£35bn, connected\n",
      "light: amplify, bridges, continuous, unveils, europe's\n",
      "\n",
      "\n",
      "\u001b[1m2225/2225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 17ms/step - loss: 0.0107\n"
     ]
    }
   ],
   "source": [
    "glove_validation_callback = ValidationCallback(valid_term_ids, glove_model, tokenizer)\n",
    "\n",
    "# Train the model for several epochs\n",
    "for ei in range(epochs):\n",
    "\n",
    "    print(f\"Epoch: {ei+1}/{epochs} started\")\n",
    "\n",
    "    news_glove_data_gen = glove_data_generator(\n",
    "        news_sequences, window_size, batch_size, n_vocab, cooc_mat\n",
    "    )\n",
    "\n",
    "    glove_model.fit(\n",
    "        news_glove_data_gen, epochs=1,\n",
    "        callbacks=glove_validation_callback,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e3e023-4c80-433e-9018-56fac76b3f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1dd002d-ed72-48b7-86d1-0b6122fec5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(model, tokenizer, vocab_size, save_dir):\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Get the words sorted according to their ID from the tokenizer\n",
    "    _, words_sorted = zip(*sorted(list(tokenizer.index_word.items()), key=lambda x: x[0])[:vocab_size-1])\n",
    "    # Add one word in front to represent the reserved ID (0)\n",
    "    words_sorted = [None] + list(words_sorted)\n",
    "\n",
    "    # Create a new array by concatenating embeddings and bias\n",
    "\n",
    "    context_embedding_weights = model.get_layer(\"context_embedding\").get_weights()[0]\n",
    "    context_embedding_bias = model.get_layer(\"context_embedding_bias\").get_weights()[0]\n",
    "    context_embedding = np.concatenate([context_embedding_weights, context_embedding_bias], axis=1)\n",
    "\n",
    "    target_embedding_weights = model.get_layer(\"target_embedding\").get_weights()[0]\n",
    "    target_embedding_bias = model.get_layer(\"target_embedding_bias\").get_weights()[0]\n",
    "    target_embedding = np.concatenate([target_embedding_weights, target_embedding_bias], axis=1)\n",
    "\n",
    "    # Save the array as pandas DataFrames\n",
    "    pd.DataFrame(\n",
    "        context_embedding,\n",
    "        index = words_sorted\n",
    "    ).to_pickle(os.path.join(save_dir, \"context_embedding_and_bias.pkl\"))\n",
    "\n",
    "    pd.DataFrame(\n",
    "        target_embedding,\n",
    "        index = words_sorted\n",
    "    ).to_pickle(os.path.join(save_dir, \"target_embedding_and_bias.pkl\"))\n",
    "\n",
    "\n",
    "save_embeddings(glove_model, tokenizer, n_vocab, save_dir='glove_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ac530d-8902-488a-822e-9c174b09617c",
   "metadata": {},
   "source": [
    "# 11. Usecase1 : Visualize word embedding for some words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "587b1979-6902-4be5-9333-0c4010e5a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the context and target embeddings\n",
    "context_embeddings = pd.read_pickle(\"C:/Users/ariji/Exploratory Data Analysis/glove_embeddings/context_embedding_and_bias.pkl\")\n",
    "target_embeddings = pd.read_pickle(\"C:/Users/ariji/Exploratory Data Analysis/glove_embeddings/target_embedding_and_bias.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74fe6b1a-4f84-4813-9a92-8c148b63a089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>None</th>\n",
       "      <td>-0.036330</td>\n",
       "      <td>-0.034749</td>\n",
       "      <td>-0.016596</td>\n",
       "      <td>-0.036723</td>\n",
       "      <td>-0.047355</td>\n",
       "      <td>-0.034517</td>\n",
       "      <td>0.021469</td>\n",
       "      <td>0.036908</td>\n",
       "      <td>-0.013375</td>\n",
       "      <td>-0.046320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040131</td>\n",
       "      <td>0.038787</td>\n",
       "      <td>0.020259</td>\n",
       "      <td>0.045287</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>-0.034098</td>\n",
       "      <td>-0.046850</td>\n",
       "      <td>-0.016506</td>\n",
       "      <td>-0.007898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>0.263443</td>\n",
       "      <td>0.165927</td>\n",
       "      <td>0.205571</td>\n",
       "      <td>0.245334</td>\n",
       "      <td>-0.397638</td>\n",
       "      <td>-0.034381</td>\n",
       "      <td>-0.247318</td>\n",
       "      <td>-0.215806</td>\n",
       "      <td>0.216790</td>\n",
       "      <td>-0.133485</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.248698</td>\n",
       "      <td>-0.095389</td>\n",
       "      <td>0.088773</td>\n",
       "      <td>-0.139337</td>\n",
       "      <td>0.505093</td>\n",
       "      <td>0.181932</td>\n",
       "      <td>0.186754</td>\n",
       "      <td>-0.393379</td>\n",
       "      <td>0.024291</td>\n",
       "      <td>0.353624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.266497</td>\n",
       "      <td>0.331774</td>\n",
       "      <td>-0.245536</td>\n",
       "      <td>0.067307</td>\n",
       "      <td>0.151394</td>\n",
       "      <td>-0.387824</td>\n",
       "      <td>-0.284053</td>\n",
       "      <td>-0.222052</td>\n",
       "      <td>0.188199</td>\n",
       "      <td>-0.048570</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145824</td>\n",
       "      <td>0.317727</td>\n",
       "      <td>0.081592</td>\n",
       "      <td>-0.191424</td>\n",
       "      <td>0.373281</td>\n",
       "      <td>0.142110</td>\n",
       "      <td>0.445748</td>\n",
       "      <td>-0.421791</td>\n",
       "      <td>-0.146392</td>\n",
       "      <td>1.005131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-0.070858</td>\n",
       "      <td>0.400328</td>\n",
       "      <td>-0.060839</td>\n",
       "      <td>-0.175254</td>\n",
       "      <td>0.384202</td>\n",
       "      <td>-0.404565</td>\n",
       "      <td>-0.314036</td>\n",
       "      <td>-0.329025</td>\n",
       "      <td>-0.186250</td>\n",
       "      <td>0.061983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101050</td>\n",
       "      <td>-0.113064</td>\n",
       "      <td>-0.141717</td>\n",
       "      <td>-0.262100</td>\n",
       "      <td>0.047011</td>\n",
       "      <td>0.345855</td>\n",
       "      <td>0.331112</td>\n",
       "      <td>-0.451555</td>\n",
       "      <td>-0.208570</td>\n",
       "      <td>0.862355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.354768</td>\n",
       "      <td>0.398066</td>\n",
       "      <td>0.206499</td>\n",
       "      <td>0.069180</td>\n",
       "      <td>0.434604</td>\n",
       "      <td>-0.195982</td>\n",
       "      <td>0.039904</td>\n",
       "      <td>-0.102387</td>\n",
       "      <td>0.203462</td>\n",
       "      <td>-0.230983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131796</td>\n",
       "      <td>0.202652</td>\n",
       "      <td>-0.239801</td>\n",
       "      <td>-0.086405</td>\n",
       "      <td>0.326264</td>\n",
       "      <td>0.361356</td>\n",
       "      <td>0.202365</td>\n",
       "      <td>-0.389407</td>\n",
       "      <td>0.335596</td>\n",
       "      <td>0.711045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claxton's</th>\n",
       "      <td>0.108052</td>\n",
       "      <td>0.047125</td>\n",
       "      <td>0.100965</td>\n",
       "      <td>0.120811</td>\n",
       "      <td>-0.134853</td>\n",
       "      <td>-0.032541</td>\n",
       "      <td>-0.072874</td>\n",
       "      <td>-0.105455</td>\n",
       "      <td>0.072872</td>\n",
       "      <td>-0.006496</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063904</td>\n",
       "      <td>0.072837</td>\n",
       "      <td>0.139502</td>\n",
       "      <td>-0.081198</td>\n",
       "      <td>0.043937</td>\n",
       "      <td>0.111830</td>\n",
       "      <td>0.050387</td>\n",
       "      <td>-0.051679</td>\n",
       "      <td>0.100905</td>\n",
       "      <td>0.089221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gears</th>\n",
       "      <td>0.008147</td>\n",
       "      <td>0.044746</td>\n",
       "      <td>0.018390</td>\n",
       "      <td>0.012532</td>\n",
       "      <td>-0.040089</td>\n",
       "      <td>-0.025723</td>\n",
       "      <td>-0.029935</td>\n",
       "      <td>0.066408</td>\n",
       "      <td>-0.021493</td>\n",
       "      <td>0.028001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031134</td>\n",
       "      <td>-0.039890</td>\n",
       "      <td>0.020140</td>\n",
       "      <td>0.026129</td>\n",
       "      <td>-0.007554</td>\n",
       "      <td>-0.005120</td>\n",
       "      <td>0.012199</td>\n",
       "      <td>-0.055415</td>\n",
       "      <td>-0.030244</td>\n",
       "      <td>0.038155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garland</th>\n",
       "      <td>0.065258</td>\n",
       "      <td>0.116206</td>\n",
       "      <td>0.033323</td>\n",
       "      <td>0.112492</td>\n",
       "      <td>-0.059957</td>\n",
       "      <td>0.084381</td>\n",
       "      <td>-0.093605</td>\n",
       "      <td>-0.023280</td>\n",
       "      <td>0.069636</td>\n",
       "      <td>-0.039847</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106903</td>\n",
       "      <td>0.011817</td>\n",
       "      <td>-0.033269</td>\n",
       "      <td>-0.032361</td>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.092133</td>\n",
       "      <td>0.040090</td>\n",
       "      <td>-0.132536</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.113452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pentathlon</th>\n",
       "      <td>0.056899</td>\n",
       "      <td>-0.004680</td>\n",
       "      <td>-0.060077</td>\n",
       "      <td>0.029075</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>-0.057718</td>\n",
       "      <td>-0.048092</td>\n",
       "      <td>-0.023860</td>\n",
       "      <td>0.012735</td>\n",
       "      <td>-0.076836</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086595</td>\n",
       "      <td>-0.028878</td>\n",
       "      <td>-0.049910</td>\n",
       "      <td>0.077126</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>-0.031748</td>\n",
       "      <td>-0.038566</td>\n",
       "      <td>-0.029229</td>\n",
       "      <td>0.084709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jeanette</th>\n",
       "      <td>0.031839</td>\n",
       "      <td>0.025373</td>\n",
       "      <td>-0.039942</td>\n",
       "      <td>-0.015307</td>\n",
       "      <td>0.037819</td>\n",
       "      <td>-0.019770</td>\n",
       "      <td>0.012630</td>\n",
       "      <td>0.010757</td>\n",
       "      <td>-0.029756</td>\n",
       "      <td>-0.048607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034077</td>\n",
       "      <td>-0.000963</td>\n",
       "      <td>-0.012860</td>\n",
       "      <td>-0.037485</td>\n",
       "      <td>0.003719</td>\n",
       "      <td>0.024114</td>\n",
       "      <td>-0.038857</td>\n",
       "      <td>-0.015136</td>\n",
       "      <td>-0.025774</td>\n",
       "      <td>0.042506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15001 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5    \\\n",
       "None       -0.036330 -0.034749 -0.016596 -0.036723 -0.047355 -0.034517   \n",
       "            0.263443  0.165927  0.205571  0.245334 -0.397638 -0.034381   \n",
       "the        -0.266497  0.331774 -0.245536  0.067307  0.151394 -0.387824   \n",
       "to         -0.070858  0.400328 -0.060839 -0.175254  0.384202 -0.404565   \n",
       "of          0.354768  0.398066  0.206499  0.069180  0.434604 -0.195982   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "claxton's   0.108052  0.047125  0.100965  0.120811 -0.134853 -0.032541   \n",
       "gears       0.008147  0.044746  0.018390  0.012532 -0.040089 -0.025723   \n",
       "garland     0.065258  0.116206  0.033323  0.112492 -0.059957  0.084381   \n",
       "pentathlon  0.056899 -0.004680 -0.060077  0.029075  0.000193 -0.057718   \n",
       "jeanette    0.031839  0.025373 -0.039942 -0.015307  0.037819 -0.019770   \n",
       "\n",
       "                 6         7         8         9    ...       119       120  \\\n",
       "None        0.021469  0.036908 -0.013375 -0.046320  ...  0.040131  0.038787   \n",
       "           -0.247318 -0.215806  0.216790 -0.133485  ... -0.248698 -0.095389   \n",
       "the        -0.284053 -0.222052  0.188199 -0.048570  ... -0.145824  0.317727   \n",
       "to         -0.314036 -0.329025 -0.186250  0.061983  ...  0.101050 -0.113064   \n",
       "of          0.039904 -0.102387  0.203462 -0.230983  ... -0.131796  0.202652   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "claxton's  -0.072874 -0.105455  0.072872 -0.006496  ... -0.063904  0.072837   \n",
       "gears      -0.029935  0.066408 -0.021493  0.028001  ...  0.031134 -0.039890   \n",
       "garland    -0.093605 -0.023280  0.069636 -0.039847  ... -0.106903  0.011817   \n",
       "pentathlon -0.048092 -0.023860  0.012735 -0.076836  ... -0.086595 -0.028878   \n",
       "jeanette    0.012630  0.010757 -0.029756 -0.048607  ...  0.034077 -0.000963   \n",
       "\n",
       "                 121       122       123       124       125       126  \\\n",
       "None        0.020259  0.045287  0.008850  0.001571 -0.034098 -0.046850   \n",
       "            0.088773 -0.139337  0.505093  0.181932  0.186754 -0.393379   \n",
       "the         0.081592 -0.191424  0.373281  0.142110  0.445748 -0.421791   \n",
       "to         -0.141717 -0.262100  0.047011  0.345855  0.331112 -0.451555   \n",
       "of         -0.239801 -0.086405  0.326264  0.361356  0.202365 -0.389407   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "claxton's   0.139502 -0.081198  0.043937  0.111830  0.050387 -0.051679   \n",
       "gears       0.020140  0.026129 -0.007554 -0.005120  0.012199 -0.055415   \n",
       "garland    -0.033269 -0.032361  0.056454  0.092133  0.040090 -0.132536   \n",
       "pentathlon -0.049910  0.077126  0.022727  0.002971 -0.031748 -0.038566   \n",
       "jeanette   -0.012860 -0.037485  0.003719  0.024114 -0.038857 -0.015136   \n",
       "\n",
       "                 127       128  \n",
       "None       -0.016506 -0.007898  \n",
       "            0.024291  0.353624  \n",
       "the        -0.146392  1.005131  \n",
       "to         -0.208570  0.862355  \n",
       "of          0.335596  0.711045  \n",
       "...              ...       ...  \n",
       "claxton's   0.100905  0.089221  \n",
       "gears      -0.030244  0.038155  \n",
       "garland     0.005027  0.113452  \n",
       "pentathlon -0.029229  0.084709  \n",
       "jeanette   -0.025774  0.042506  \n",
       "\n",
       "[15001 rows x 129 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e6447d7-981d-4f25-8144-001f587e76e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6031"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2262bb0a-ae6d-4c7d-907b-eb7a2347b48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.065112\n",
       "1      0.152522\n",
       "2     -0.004471\n",
       "3     -0.192605\n",
       "4      0.054866\n",
       "         ...   \n",
       "124    0.035825\n",
       "125    0.171777\n",
       "126   -0.100865\n",
       "127    0.001464\n",
       "128    0.200592\n",
       "Name: dog, Length: 129, dtype: float32"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_embeddings.iloc[6031]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0896c33-e6b5-46a9-9099-e059d6476bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([        None,           '',        'the',         'to',         'of',\n",
      "              'and',          'a',         'in',        'for',         'is',\n",
      "       ...\n",
      "       'meticulous',   'olympian', 'decathlete',    'findlay',   'stumbled',\n",
      "        'claxton's',      'gears',    'garland', 'pentathlon',   'jeanette'],\n",
      "      dtype='object', length=15001)\n"
     ]
    }
   ],
   "source": [
    "print(context_embeddings.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68f74698-e7b6-4665-9368-eb5cd4462760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6031"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f6226d4-a301-4149-bc97-d1d50cd4c324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "0      0.065112\n",
      "1      0.152522\n",
      "2     -0.004471\n",
      "3     -0.192605\n",
      "4      0.054866\n",
      "         ...   \n",
      "124    0.035825\n",
      "125    0.171777\n",
      "126   -0.100865\n",
      "127    0.001464\n",
      "128    0.200592\n",
      "Name: dog, Length: 129, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# example 1 : word vector for \"dog\"\n",
    "#GloVe_dog = glove_model.get_layer(\"context_embedding\").get_weights()[0][tokenizer.word_index[\"dog\"]]\n",
    "GloVe_dog_embedding = context_embeddings.iloc[tokenizer.word_index['dog']]\n",
    "\n",
    "print(len(GloVe_dog_embedding))\n",
    "print(GloVe_dog_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "210bf0b1-c41c-40ed-b153-b16cfb83a5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "0      0.034035\n",
      "1      0.093885\n",
      "2      0.043728\n",
      "3      0.101932\n",
      "4      0.065371\n",
      "         ...   \n",
      "124   -0.027784\n",
      "125    0.054978\n",
      "126    0.029087\n",
      "127    0.124382\n",
      "128    0.196347\n",
      "Name: cat, Length: 129, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# example 2 : word vector for \"cat\"\n",
    "#GloVe_cat = glove_model.get_layer(\"context_embedding\").get_weights()[0][tokenizer.word_index[\"cat\"]]\n",
    "GloVe_cat_embedding = context_embeddings.iloc[tokenizer.word_index['cat']]\n",
    "\n",
    "print(len(GloVe_cat_embedding))\n",
    "print(GloVe_cat_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d077e31f-f141-4e32-9d2f-d49e1f45e9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "0      0.079416\n",
      "1     -0.076226\n",
      "2      0.000903\n",
      "3      0.184338\n",
      "4      0.111428\n",
      "         ...   \n",
      "124    0.233430\n",
      "125    0.035179\n",
      "126    0.170419\n",
      "127   -0.130050\n",
      "128    0.287434\n",
      "Name: man, Length: 129, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# example 3 : word vector for \"man\"\n",
    "#GloVe_man = glove_model.get_layer(\"context_embedding\").get_weights()[0][tokenizer.word_index[\"man\"]]\n",
    "GloVe_man_embedding = context_embeddings.iloc[tokenizer.word_index['man']]\n",
    "\n",
    "print(len(GloVe_man_embedding))\n",
    "print(GloVe_man_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d79343e-a282-4233-bb73-d517326790ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "0     -0.046141\n",
      "1      0.131601\n",
      "2     -0.055070\n",
      "3     -0.157930\n",
      "4      0.164094\n",
      "         ...   \n",
      "124   -0.042518\n",
      "125   -0.024443\n",
      "126   -0.074304\n",
      "127   -0.271653\n",
      "128    0.252643\n",
      "Name: woman, Length: 129, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# example 4 : word vector for \"woman\"\n",
    "#GloVe_woman = glove_model.get_layer(\"context_embedding\").get_weights()[0][tokenizer.word_index[\"woman\"]]\n",
    "GloVe_woman_embedding = context_embeddings.iloc[tokenizer.word_index['woman']]\n",
    "\n",
    "print(len(GloVe_woman_embedding))\n",
    "print(GloVe_woman_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf38f43-2160-4013-8063-0a9c53a0d212",
   "metadata": {},
   "source": [
    "# 12. Usecase2 : Similarity of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07de919e-00ef-4542-9978-9b932351086a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40173376\n"
     ]
    }
   ],
   "source": [
    "# example 1 : similarity score between dog and cat\n",
    "similarity = np.dot(GloVe_dog_embedding, GloVe_cat_embedding) / (np.linalg.norm(GloVe_dog_embedding) * np.linalg.norm(GloVe_cat_embedding))\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5c8537b-c362-49d9-b7a1-f415fd3f4fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20789835\n"
     ]
    }
   ],
   "source": [
    "# example 2 : similarity score between dog and man\n",
    "similarity = np.dot(GloVe_dog_embedding, GloVe_man_embedding) / (np.linalg.norm(GloVe_dog_embedding) * np.linalg.norm(GloVe_man_embedding))\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebba03f8-ed6b-4f5c-86e3-463d51af6463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13812399\n"
     ]
    }
   ],
   "source": [
    "# example 3 : similarity score between woman and man\n",
    "similarity = np.dot(GloVe_man_embedding, GloVe_woman_embedding) / (np.linalg.norm(GloVe_man_embedding) * np.linalg.norm(GloVe_woman_embedding))\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8ba79f-bfa0-4eb1-91f7-118b550674eb",
   "metadata": {},
   "source": [
    "# 13. Usecase3 : Analogy task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11b93910-dfb1-48e7-bf43-a0d80f70ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "king_vector = glove_model.get_layer(\"context_embedding\").get_weights()[0][tokenizer.word_index[\"king\"]]\n",
    "man_vector = glove_model.get_layer(\"context_embedding\").get_weights()[0][tokenizer.word_index[\"man\"]]\n",
    "woman_vector = glove_model.get_layer(\"context_embedding\").get_weights()[0][tokenizer.word_index[\"woman\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dea771d-5877-46d4-8d95-df39f4659309",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Queen'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mword_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueen\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Queen'"
     ]
    }
   ],
   "source": [
    "tokenizer.word_index[\"Queen\"] ## doesnn't exist in the word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d443a38-e939-4f9e-b83b-411e8ba6a971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "[-0.1072585   0.25532705 -0.16392715 -0.59597886 -0.10661711  0.09065117\n",
      "  0.1273671   0.08389243  0.40643725 -0.11784977 -0.19390135  0.46458665\n",
      "  0.23958902 -0.29843414  0.13671944  0.33693662 -0.1761643   0.11873163\n",
      "  0.0778905   0.11633837 -0.42010018 -0.23044534 -0.481146   -0.3766451\n",
      "  0.12563775  0.5352857   0.05852748  0.16316634  0.33384562  0.4587633\n",
      "  0.11799078 -0.18976457  0.19446024  0.19190978  0.13035901  0.18002236\n",
      " -0.26145184 -0.02780539  0.21991275 -0.3134792   0.13610429 -0.2726212\n",
      " -0.06337574 -0.01761575  0.04404603 -0.11970913  0.33556253  0.31643936\n",
      "  0.10766982  0.28283274  0.2290895  -0.34123135 -0.11084142 -0.38804632\n",
      " -0.12532598  0.4942302  -0.17592773 -0.0310415  -0.4120087   0.5502757\n",
      "  0.09279455  0.33266354 -0.06980092 -0.01942305 -0.357976    0.46552268\n",
      "  0.10120413  0.0862118   0.35329393 -0.1342226   0.11715373  0.05425069\n",
      "  0.19657649 -0.11257687  0.23719537 -0.30711657  0.21943654  0.32002527\n",
      " -0.43137053  0.23663983 -0.2469813   0.2050604  -0.31264672 -0.07905117\n",
      "  0.21971467  0.30563843  0.26373792  0.08170373  0.05506755  0.03946695\n",
      "  0.07145822  0.34752855  0.301301   -0.11449627 -0.02261039 -0.188789\n",
      "  0.13192955  0.18220119  0.00146176  0.110834   -0.3300984   0.47987452\n",
      " -0.03751596  0.53029287 -0.153946   -0.24268447 -0.3550797   0.1746421\n",
      "  0.20282507  0.18037051  0.4136233  -0.2322506   0.13463008 -0.05577498\n",
      " -0.29118115  0.06839173  0.1845045   0.0501603   0.52035695  0.20954129\n",
      " -0.14751954  0.277588   -0.18979332  0.35921663 -0.03518945  0.08473874\n",
      " -0.37140328 -0.08164333]\n"
     ]
    }
   ],
   "source": [
    "# application of analogy to calculate Queen vector\n",
    "\n",
    "queen_vector = king_vector - man_vector + woman_vector\n",
    "print(len(queen_vector))\n",
    "print(queen_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb405114-5b83-4856-abe4-fe49346223ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4108e1-5508-4e80-9828-662870c6c19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01145b44-67cc-4987-a193-51799766c65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741bfc5-19fc-4815-83a4-c8a05922a4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a3dc0e-515a-488d-9afd-e0729a15832c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df4baf-de8e-4a54-9c53-a4a179344188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937fe1f9-ee8d-4b07-8fad-434da1e88e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a68fd27-7d75-4a38-88e1-c4545631f46d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
